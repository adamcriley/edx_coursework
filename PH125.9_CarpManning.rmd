---
title: "PH125.9x Data Science Capstone"
subtitle: "Predicting Federal District Court Outcomes"
author: "Adam C. Riley"
date: "11/30/2020"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 4
    number_sections: yes
Theme: cosmo
linkcolor: blue
highlight: haddock
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, fig.width=7.5, fig.height=5, cache.extra=61)
```
\
The R Markdown code used to generate this pdf document and associated files can be found my [Github](https://github.com/adamcriley/edx_coursework).\

# Executive Summary  

## Artificial Intelligence in the Legal Field  

Artificial Intelligence is booming in the legal field as firms scramble to research more quickly and leverage technology to provide their clients with a more complete experience. Any edge that a firm can bring to the litigation landscape become a crucial factor in determining how they negotiate, allocate resources, and communicate with their clients.\
\
This project will focus on assisting legal practitioners to predict the outcome of cases with social impact components. This litigation is often costly and can take years to complete. With predictive information, applied to the facts of their particular issue, they can either temper the expectations of a plaintiff with a case that will not likely be ruled in their favor or to be more aggressive with a defendant that is facing a potential loss and is looking for a way to mitigate the impact of the associated monetary award.\
\
We are not aiming to create a model that can replace legal judgment, but will focus on a tool that can be utilized alongside traditional legal research to provide additional insights.\

## Data  

Robert A. Carp (Professor of Political Science at the University of Houston) and Kenneth L. Manning (Professor of Political Science at the University of Massachusetts-Dartmouth) examined and collected a subset of the judicial opinions published in the *Federal Supplement* that contained a question of law with a traditional liberal-conservative dimension. The database contains more than 110,000 entries decided between 1927 and 2012. It is estimated that (as of 2016) approximately half of the decisions published in the *Federal Supplement* have been included in the collection.\
\
The authors would read each decision and, if appropriate, coded various demographic information and cataloged the decision as either liberal or conservative using criteria that can be found in the database's [companion codebook](https://www.umassd.edu/media/umassdartmouth/political-science/facultydocs/codebook-5-24-2016.pdf). If a case could not clearly be classified as either liberal or conservative of if there were multiple issues or multiple parties that made the determination uncertain, the case was not included in the database.\

### Dependent Variable  

The dependent variable in the data is the ordinal-level liberal-conservative classification, "libcon." The variable has a value of either 0 (Conservative) or 1 (Liberal) and takes the form of a Bernoulli distribution.\

### Independent Variables  

Independent variables include details about the judge authoring the opinion, information about the court, topical classifications about the subject matter of the case, and the date of ruling. Each of the independent variables has been encoded but can be decoded using the above referenced companion codebook. We will look further into each of the independent variables as we inspect and process the data below.\

## Process  

I will follow the following steps to create the model for this project:

1. Data Analysis
    1. Data Import & Cleaning
    1. Data Exploration & Visualization
    1. Analysis Conclusion
1. Supervised Learning Methods
    1. Modeling Approach
    1. Models & Algorithms
1. Results
    1. Metrics
    1. Performance
1. Conclusion

## Goal  

This Capstone Project will use supervised machine learning to create a classifier based on data contained in the Carp Manning U.S. District Court Database that predicts the outcome of a certain United States Federal District Court cases as either liberal or conservative.\

# Data Analysis  

## Data Import & Cleaning  

```{r include=FALSE}
if(!require(plyr)) install.packages("plyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(httr)) install.packages("httr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(pdftools)) install.packages("pdftools", repos = "http://cran.us.r-project.org")
if(!require(usmap)) install.packages("usmap", repos = "http://cran.us.r-project.org")
if(!require(treemapify)) install.packages("treemapify", repos = "http://cran.us.r-project.org")
```
The Carp-Manning data is split between two different documents, so we need to import them both and combine them to be able to effectively explore the data in a meaningful way. The first document is the coded data set (we will utilize the excel version, although the data is available in several formats) and the second is the PDF companion codebook which we will use to decode and map the data to make it readable.\
```{r include=FALSE}
##############
# Import Data
##############

# Check to see if the file(s) have been downloaded to the working directory
# If the files exist and are readable, import them from the local copy
# If there are any problem, fail over to online source(s)

# Excel file with core data
file_path <- "FedCourtDatabase.xlsx"
file_url <- "https://www.umassd.edu/media/umassdartmouth/political-science/facultydocs/fdcdata_thru_2012_n=110977.xlsx"
CMget <- try(read_excel(file_path))
if (file.exists(file_path) && !inherits(CMget, "try-error")) {
    CM <- read_excel(file_path)
} else {
    GET(file_url, write_disk(dl <- tempfile(fileext = ".xlsx")))
    CM <- read_excel(dl)
}

# PDF file with reference tables and data information
pdf_path <- "DatabaseManual.pdf"
pdf_url <- "https://www.umassd.edu/media/umassdartmouth/political-science/facultydocs/codebook-5-24-2016.pdf"
PDFget <- try(CM_text <- pdf_text(pdf_path))
if (file.exists(pdf_path) && !inherits(PDFget, "try-error")) {
    CM_text <- pdf_text(pdf_path)
} else {
    CM_text <- pdf_text(pdf_url)
}
```
We will do a quick check of our files to see if there are locally available copies of these files and if there are not, we will automatically download them from the the [University of Massachusetts-Dartmouth project site](https://www.umassd.edu/cas/polisci/resources/us-district-court-database/).  The excel file can be imported directly as a tribble and the PDF document is initially converted into a character vector.\
\
Now that we have both documents, we need to process the codebook and scrape the the tables within to be able to manipulate them. The first step is to inspect the codebook and identify the tables we want to target and note what pages they appear on. While doing that, I noticed that each row of each table is on its own line and each entry appears to be in similar format: a numerical identifier, then a dash, then decoded information.  After cross referencing to check that the values within the codebook match exactly to the raw excel data and verifying the formats, everything looks promising. We can make a notes based on our visual inspect and can then create a function that will scan the pages we identified and collect the table information from them.\

### Tables found in the codebook:  

**crtpoint**\
Encoded Format: three numbers\
Pages: 5, 6, 7, 8, 9, 10, 11, 12, 13\
\
**circuit**\
Encoded Format: two numbers\
Page: 15\
\
**state**\
Encoded Format: two numbers\
Pages: 16, 17\
\
**statdist**\
Encoded Format: three numbers\
Pages: 17, 18, 19, 20\
\
**casetype**\
Encoded Format: two numbers\
Pages: 24, 25\
\
**category**\
Encoded Format: one number\
Page: 39\
\
**appres**\
Encoded Format: two number\
Page: 43\
\
**demographics**\
Encoded Format: one numbers\
Page: 44\
Note: There are several tables on page 44 and we'll have to split the data in further processing\
```{r include=FALSE}
# Target page numbers for each reference table
CRTPOINT_pages <- c(5,6,7,8,9,10,11,12,13)
CIRCUIT_pages <- c(15)
STATE_pages <- c(16, 17)
STATDIST_pages <- c(17, 18, 19, 20)
CASETYPE_pages <- c(24, 25)
CATEGORY_pages <- c(39)
APPRES_pages <- c(43)
DEMOGRAPHIC_pages <- c(44)

# Identify data format for each reference table
CRTPOINT_regex <- "^\\d\\d\\d"
CIRCUIT_regex <- "^\\d\\d"
STATE_regex <- "^\\d\\d"
STATDIST_regex <- "^\\d\\d\\d"
CASETYPE_regex <- "^\\d\\d"
CATEGORY_regex <- "^\\d"
APPRES_regex <- "^\\d\\d"
DEMOGRAPHIC_regex <- "^\\d"
```
We are really lucky that there is minimal cross over between the pages. I noted above in the demographics block about page 44, but there are no other instances that will require further processing because where there are two tables on one page, the formats are different and can be parsed automatically.\
\
I initially envisioned this as a three stage process:\
\
Stage one: Split the character vector text by line to create the rows of the table\
Stage two: split the rows using the dash to create the columns\
Stage three: verify the data and return it\
\
After some trial and error, I realized that the 'dash' character was not being scanned in a uniform way and needed to be cleaned so that any horizontal line between the formatted number and the definition would be converted to a 'dash' and stage two would work as intended. I also needed to strip white space from the beginning and end of the strings to make sure the verification at stage three matched as I expected. Finally, I also removed any rows with empty values.\
\
We will name our function 'get_CM_tables':\
```{r}
get_CM_tables <- function(CM_page, CM_format) {
    stage0 <- gsub("\\s[[:punct:]]+\\s)", " - ", CM_text[[CM_page]])
    stage0 <- gsub(" â€“ ", " - ", CM_text[[CM_page]])
    stage1 <- str_split(stage0, "\n", simplify=TRUE)
    stage2 <- str_split(stage1, " - ", simplify=TRUE)
    stage2[,] <- trimws(stage2[,])
    stage3 <- stage2[which(grepl(CM_format, stage2[,1])),]
    stage3 <- as.data.frame(stage3[which(stage3[,2] != ''),])
    return(stage3)
}
```
Then we will loop over the function for each table using a vector of page numbers and a regular expression for the format of the number we are trying to decode. Once processed, we will combine the processed results from each page into a single table and then rename the columns using more descriptive titles.\
```{r include=FALSE}
CRTPOINT_table <- lapply(CRTPOINT_pages, get_CM_tables, CM_format=CRTPOINT_regex)
CRTPOINT_table <- ldply(CRTPOINT_table, data.frame)
colnames(CRTPOINT_table) <- c("crtpoint", "location")

CIRCUIT_table <-lapply(CIRCUIT_pages, get_CM_tables, CM_format=CIRCUIT_regex)
CIRCUIT_table <- ldply(CIRCUIT_table, data.frame)
colnames(CIRCUIT_table) <- c("circuit", "courtofappeals")

STATE_table <- lapply(STATE_pages, get_CM_tables, CM_format=STATE_regex)
STATE_table <- ldply(STATE_table, data.frame)
colnames(STATE_table) <- c("state", "statename")

STATDIST_table <- lapply(STATDIST_pages, get_CM_tables, CM_format=STATDIST_regex)
STATDIST_table <- ldply(STATDIST_table, data.frame)
colnames(STATDIST_table) <- c("statdist", "districtcourt")

CASETYPE_table <- lapply(CASETYPE_pages, get_CM_tables, CM_format=CASETYPE_regex)
CASETYPE_table <- ldply(CASETYPE_table, data.frame)
colnames(CASETYPE_table) <- c("casetype", "typeofcase")

CATEGORY_table <- lapply(CATEGORY_pages, get_CM_tables, CM_format=CATEGORY_regex)
CATEGORY_table <- ldply(CATEGORY_table, data.frame)
colnames(CATEGORY_table) <- c("category", "generaltypeofcase")

APPRES_table <- lapply(APPRES_pages, get_CM_tables, CM_format=APPRES_regex)
APPRES_table <- ldply(APPRES_table, data.frame)
colnames(APPRES_table) <- c("appres", "presname")

#This is outside data I mapped because I was interested
APPRES_table$prespartyname <- ifelse(APPRES_table$appres %in% c(25, 26, 27, 29, 30, 31, 34, 37, 38, 40, 41, 43), "Republican", "Democrat")

# Demographics were initially treated as a group because all the information was on a single page
# After processing, the collections were parsed into individual tables
DEMOGRAPHIC_table <- lapply(DEMOGRAPHIC_pages, get_CM_tables, CM_format=DEMOGRAPHIC_regex)
DEMOGRAPHIC_table <- ldply(DEMOGRAPHIC_table, data.frame)

PARTY_table <- DEMOGRAPHIC_table[1:3,]
colnames(PARTY_table) <- c("party", "judgeparty")

GENDER_table <- DEMOGRAPHIC_table[4:5,]
colnames(GENDER_table) <- c("gender", "judgegender")

RACE_table <- DEMOGRAPHIC_table[6:10,]
colnames(RACE_table) <- c("race", "judgerace")

detach(package:plyr)
```
These tables can be joined to the raw excel data to create a master data set with all Carp-Manning data.\

### Non-Carp-Manning Data  

I added two additional variables to the table during processing. The first was a calculation of the difference between the year the judge was appointed and the the year the case opinion was written. My gut is tells me that the length of time a jurist has served on the bench may change how they rule either from the impact of age or additional experience. Second, I looked up and added the political party of the president that appointed the judge. This was done completely out of curiosity.\
\
There are a lot of other additional classifiers that could be added to this data to improve the fit and accuracy of the model. States could be assigned different regions (which could be distinct from circuits), case types could be further sub-divided based on topic or assigned into sub-categories, or more demographic information could be added about the judges. These steps could add value, especially subdividing the topics, and should be explored if the model would have a commercial application which would require a very high degree of accuracy based on the business case (as described above).\
```{r include=FALSE}
# Set up table for matching from reference material
CM <- CM %>% mutate(
    judge=as.factor(judge), 
    crtpoint=as.factor(sprintf('%03d', crtpoint)), 
    circuit=as.factor(sprintf('%02d', circuit)), 
    state=as.factor(sprintf('%02d', state)), 
    statdist=as.factor(sprintf('%03d', statdist)), 
    month=as.integer(month), 
    year=as.integer(year), 
    libcon=as.integer(libcon), 
    casetype=as.factor(sprintf('%02d', casetype)), 
    category=as.factor(category), 
    apyear=as.integer(apyear), 
    appres=as.factor(sprintf('%02d', appres)), 
    party=as.factor(party), 
    race=as.factor(race) , 
    gender=as.factor(gender),
    tenure=(as.integer(year) - as.integer(apyear))
    )

# Match and add reference information
CM_master <- CM %>% 
    left_join(CRTPOINT_table, by="crtpoint") %>%
    left_join(CIRCUIT_table, by="circuit") %>%
    left_join(STATE_table, by="state") %>%
    left_join(STATDIST_table, by="statdist") %>%
    left_join(CASETYPE_table, by="casetype") %>%
    left_join(CATEGORY_table, by="category") %>%
    left_join(APPRES_table, by="appres") %>%
    left_join(PARTY_table , by="party") %>%
    left_join(GENDER_table , by="gender") %>%
    left_join(RACE_table, by="race")

CM_master$presparty <- ifelse(CM$appres %in% c(25, 26, 27, 29, 30, 31, 34, 37, 38, 40, 41, 43), 0, 1)

```
We have the all the data compiled in a single table, now we need to partition the data to hold out a small portion for unbiased final testing to allow us to determine how accurate our predictions are against data that was never involved in the training or exploration and will remain unknown to both myself and the algorithm. Although there is no hard rule about the amount of data that should be selected for a testing hold out, typical values are between 10-20%. I will select 15% of the data and set it aside because while the data set isn't large, there are several parameters that will need to be tuned. We will train the models on the remaining 85%.\
```{r include=FALSE}
###################################
# Determine Test and Training Sets
###################################

# Test set will be a 15% holdout the of Carp-Manning data
set.seed(61, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = CM$libcon, times = 1, p = 0.15, list = FALSE)
CMtraining <- CM_master[-test_index,]
CMtemp <- CM_master[test_index,]

# Make sure "judge" and "casetype" in test set are also in CMtraining set
CMtest <- CMtemp %>% 
      semi_join(CMtraining, by = "judge") %>%
      semi_join(CMtraining, by = "typeofcase")

# Add rows removed from test set back into CMtraining set
removed <- anti_join(CMtemp, CMtest)

CMtraining <- rbind(CMtraining, removed)

# Clean up
rm(removed, CMget, dl, CMtemp, test_index, file_path, file_url, pdf_url, CM_text, DEMOGRAPHIC_table, CRTPOINT_pages, CIRCUIT_pages, STATE_pages, STATDIST_pages, CASETYPE_pages, CATEGORY_pages, APPRES_pages, DEMOGRAPHIC_pages, CRTPOINT_regex, CIRCUIT_regex, STATE_regex, STATDIST_regex, CASETYPE_regex, CATEGORY_regex, APPRES_regex, DEMOGRAPHIC_regex, PDFget, pdf_path, CRTPOINT_table, CIRCUIT_table, STATE_table, STATDIST_table, CASETYPE_table, CATEGORY_table, APPRES_table, PARTY_table, GENDER_table, RACE_table)
```
## Data Exploration & Visualization  

We should begin our exploration by taking a look at how our independent variables influence our dependent one.\
```{r}
glimpse(CMtraining)
```
The training set consists of 94,340 entries of 30 columns (11 of which are duplicate and a result of the decoding process). The average case is outcome is a conservative one (~58% of the time). With 19 independent variables to consider, we need to visually inspect how our outcome is being influenced. Many of our independent variables might be related, so we will briefly look at each group.\

### Jurisdiction  

Our first chart will take a look at jurisdiction to see if the state the case in which the case was decided is significant, the hypothesis is that there is a large impact. Different states are bound to follow different precedent and each state can have different laws, both of which could change the outcomes one way or the other:\
```{r echo=FALSE}
tend_state <- CMtraining %>% group_by(statename) %>% summarise(avgyear = (mean(libcon)-.5))
colnames(tend_state) <- c("state", "libcon")
plot_usmap(data = tend_state, values = "libcon", color="white") +  scale_fill_continuous(name= "", low = "lightpink2", high = "dodgerblue", label=c("v.cons", "cons", "m.con", "m.lib", "lib", "v.lib")) + theme(legend.position = "right")
```
The are a lot of different shades of color in that representation, but there are a couple of states that stand out. Additionally, as we look into the data more closely, we can notice that there are several US territories that were not represented on the original map. We should take a different look and switch to a view that will allow us see a more direct comparison between all jurisdictions.\
```{r echo=FALSE}
tend_state$tend <- ifelse(tend_state$libcon > 0, "liberal", "conservative")
tend_state <- tend_state[order(tend_state$libcon), ]
tend_state$state <- factor(tend_state$state, levels=tend_state$state)
ggplot(data=tend_state, aes(x=state, y=libcon, label=libcon)) + geom_bar(stat='identity', aes(fill=tend)) +  scale_fill_manual(name= "", labels = c("Conservative", "Liberal"), values = c("liberal"="dodgerblue", "conservative"="lightpink2")) + ylab("") + xlab("") + scale_y_continuous(label=c("cons", "m.cons", "ind", "m.lib", "lib", "v.lib")) + coord_flip() + theme_classic() + theme(legend.position="none")
rm(tend_state)
```
As expected, jurisdiction seems to be an important independent variable, which matches exactly with the expectation of anyone with legal training.\
\newpage

### Judge Demographics  

Another obvious set of variables that could influence the outcome of a given case are the demographic specifics of the judge presiding over the matter. This group of visualizations will be summarized so that the average for each characteristic will be shown for a given year and each characteristics will be plotted over time. This significantly simplifies what is shown.\
\
The first demographic we will examine is race:\
```{r echo=FALSE}
# race
# the data here is too skewed to be amenable to smooth trend lines, picking linear instead
tend_race <- CMtraining %>% group_by(judgerace, year) %>% summarise(avgyear = (mean(libcon)-.5))
tend_race$judgerace <- ifelse(tend_race$judgerace== "African-American/black", "Af.Amer", ifelse(tend_race$judgerace == "white/Caucasian", "C.Amer", ifelse(tend_race$judgerace == "Latino/Hispanic", "L.Amer", ifelse(tend_race$judgerace =="Native American", "N.Amer" , ifelse(tend_race$judgerace== "Asian American", "As.Amer", "N/A")))))
ggplot(data= tend_race, aes(year, avgyear, color=judgerace)) + geom_point() + geom_smooth(method="glm", se=FALSE) + labs(x="Year", y="") + geom_smooth(method="loess", se=FALSE) + scale_y_continuous(labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + scale_colour_manual(name="", values = c("Af.Amer"="dodgerblue", "C.Amer"="lightpink2", "L.Amer"="palegreen3", "N.Amer" = "gold", "As.Amer" = "grey45" )) + theme_classic()
rm(tend_race)
```
There are five possible values for race, which I abbreviated in the above figure: "African-American/black" ("Af.Amer"), "white/Caucasian" ("C.Amer"), "Latino/Hispanic" ("L.Amer"), "Native American" ("N.Amer"), and "Asian American" ("As.Amer"). While I had planned to add smoothed trend lines to all the graphs, some of the race data was too skewed for smoothing and linear approximations were required. There are clear trends contained in the racial breakdown so it is a trait that will likely add value to our model.\
\newpage
Next, we will examine gender:\
```{r echo=FALSE}
# gender
tend_gender <- CMtraining %>% group_by(judgegender, year) %>% summarise(avgyear = (mean(libcon)-.5))
ggplot(data= tend_gender, aes(year, avgyear, color=judgegender)) + geom_point() + geom_smooth(method="loess", se=FALSE) + labs(x="Year", y="") + scale_y_continuous(labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + scale_colour_manual(name="", values = c("female"="dodgerblue", "male"="lightpink2")) + theme_classic()
```
The Carp-Manning authors have used the binary "male" and "female" designations for this study. Again, there is a strong trend visible with this data, even though the data has some variability in the first half of the 20th century. Actually if we look at the graph with confidence intervals displayed (next page), we can see that while the data for the male judges seems pretty steady, data for the female judges has a lot more uncertainly. This is probably due in large part to the smaller number of female judges, but I think the trend is worth adding to the model regardless.\
```{r echo=FALSE}
ggplot(data= tend_gender, aes(year, avgyear, color=judgegender)) + geom_point() + geom_smooth(method="loess", se=TRUE, aes(fill=judgegender)) + labs(x="Year", y="") + scale_y_continuous(labels=c("vcons","cons", "m.cons", "ind", "m.lib", "lib")) + scale_colour_manual(name="", values = c("female"="dodgerblue", "male"="lightpink2")) + scale_fill_manual(name="", values = c("female"="dodgerblue", "male"="lightpink2")) + theme_classic()
rm(tend_gender)
```
\newpage
The final piece of judge demographic information we should explore is the correlation of the ruling tendency and the judge's political party:\
```{r echo=FALSE}
# party affiliation
tend_party <- CMtraining %>% group_by(judgeparty, year) %>% summarise(avgyear = (mean(libcon)-.5))
tend_party <- tend_party[complete.cases(tend_party), ] # Removing data with party designation "99"
tend_party$judgeparty <- ifelse(tend_party$judgeparty=="Independent/Other/Unknown", "Ind", ifelse(tend_party$judgeparty=="Democrat", "Dem", ifelse(tend_party$judgeparty=="Republican", "Rep", "N/A")))
ggplot(data= tend_party, aes(year, avgyear, color=judgeparty)) + geom_point() + geom_smooth(method="loess", se=FALSE) + labs(x="Year", y="") + scale_y_continuous(labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + scale_colour_manual(name="", values = c("Dem"="dodgerblue", "Rep"="lightpink2", "Ind"="palegreen3")) + theme_classic()
rm(tend_party)
```
There were 4 designations provided by the authors for this characteristic: "Democrat" ("Dem"), "Republican" ("Rep"), "Independent/Other/Unknown" ("Ind"), and "99". I am not sure, nor do the authors indicate what the the "99" designation is associated with, so I removed that factor from the chart.\
\
This display matches with expectations almost perfectly, with Democrats being traditionally being perceived as more liberal and Republican as being more conservative over this time frame. Additionally, Independents seem not to follow a specific trend which is also an expected pattern. These trends make common sense as political party is a feature personally chosen by the judge and it logically follows that the expression of the political party would also provide good information about the ruling tendencies of each judge.\
\newpage

### Appointment Demographics  

Federal judges of all levels in the United States are nominated by the President and confirmed by the Senate. While this action gets a lot of press coverage for high level appointments, I suspect there is just as much of an impact for lower court judges as well:\
```{r echo=FALSE}
# pres by name
tend_pres <- CMtraining %>% group_by(presname, year, judge) %>% summarise(avgyear = (mean(libcon)-.5), count=n())
ggplot(tend_pres, aes(x=year, y=avgyear, color=presname)) + geom_point() + labs(x="Year", y="") + scale_y_continuous(name="", labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + theme_classic()
rm(tend_pres)
```
This visualization is too crowded to be able to make sense of and no trends are apparent from a quick examination of the graph. The are so many presidents listed that adding trend lines makes the graph more confusing and even removing the data points all together so as to just leave the trend lines was still too much. We should peel the information back a layer and group the presidents by their political party and see if a trend emerges. I feel like there should be some sort of correlation, as presidents would be more likely to appoint judges that have views similar to their own:\
```{r echo=FALSE}
# pres by party
tend_pres <- CMtraining %>% group_by(prespartyname, year) %>% summarise(avgyear = (mean(libcon)-.5), count=n())
ggplot(tend_pres, aes(x=year, y=avgyear, color=prespartyname)) + geom_point() + labs(x="Year", y="") + geom_smooth(method="loess", se=FALSE) + scale_y_continuous(labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + scale_colour_manual(values = c("Democrat"="dodgerblue", "Republican"="lightpink2")) + scale_color_discrete(name = "") + theme_classic()
rm(tend_pres)
```
We can now see a clear trend in the data and the shape of our trend lines mirrors the trend lines of the judge's political party. This also makes common sense and tells us that the appointing president designation might be conflated because presidents are likely to pick judges with political parties matching their own. We need to be caution of that when selecting variables for our model.\
\newpage

### Tenure  

We can also test the hypothesis that length of time in office changes the tendency of a judge to rule in a certain way:\
```{r echo=FALSE, cache=TRUE}
# tenure
tend_duration <- CMtraining %>% group_by(tenure, judge) %>% summarise(avgyear = (mean(libcon)-.5), count=n())
tend_duration <- tend_duration[tend_duration$tenure > -1, ]
tend_duration$sign <- ifelse(tend_duration$avgyear < 0, -1, 1)
tend_duration$count <- tend_duration$count*tend_duration$sign
ggplot(tend_duration, aes(x=tenure, y=avgyear)) + geom_point(aes(color= avgyear, size=abs(count))) + scale_color_gradient(low="lightpink2", high="dodgerblue") + scale_y_continuous(labels=c("cons", "m.cons", "ind", "m.lib", "lib")) + geom_smooth(method="loess", color="purple", fill="plum1") + labs(x="Years in Office", y="") + theme_classic() + theme(legend.position="none") 
rm(tend_duration)
```
On this chart, each dot represents a group of judges at that particular ideological lean for that number of years in office. The size of the dot represents the number of opinions written by the group. We do see a slight conservative shift after about 15 years on the bench, but before that we see a slight upward trend, which I did not expect. This measure does look impactful.\
\newpage

### Subject Matter  

The final independent variables we want to take a look at involves the subject matter of the law suit:\
```{r echo=FALSE}
# case type distribution
treeMap <- CMtraining %>% group_by(generaltypeofcase, typeofcase) %>% summarise(avgtype = (mean(libcon)-.5), count=n())
treeMap$tend <- ifelse(treeMap$avgtype < 0, "conservative", "liberal")
treeMap$generaltypeofcase <- str_replace(treeMap$generaltypeofcase, " case", "")
ggplot(data=treeMap, aes(area = count, fill=avgtype, label=typeofcase, subgroup=generaltypeofcase)) + geom_treemap() + geom_treemap_subgroup_border(color="black") + geom_treemap_subgroup_text(color="black") + geom_treemap_text(fontface = "italic", color = "white", place = "topleft", reflow = TRUE) + scale_fill_continuous(low = "lightpink2", high = "dodgerblue", name = "", label=c("cons", "m.cons", "ind", "m.lib", "lib")) + theme_classic()
rm(treeMap)
```
The tree diagram is useful here because it allows us to take a look at the variance from each of the 31 case type designations while keeping them grouped within the 3 case type categories. There are noticeable trends here, which is expected because of the style of visualization, but if we view the overall categories as a whole over time, we can see that they seem to have a stronger effect:\
```{r echo=FALSE}
# category
trend_cat <- CMtraining %>% group_by(generaltypeofcase, year) %>% summarise(avgtype = (mean(libcon)-.5), count=n())
trend_cat$generaltypeofcase <- str_replace(trend_cat$generaltypeofcase, " case", "")
ggplot(trend_cat, aes(x=year, y=avgtype, color=generaltypeofcase)) + geom_point() + geom_smooth(method="loess", se=FALSE) + labs(x="Year", y="") + scale_y_continuous(labels=c("conservative", "m. conservative", "neutral", "m. liberal", "liberal")) + scale_colour_manual(name="", values = c("Economic Regulation and/or Labor"="dodgerblue", "Criminal Justice"="lightpink2", "Civil Liberties/Rights"="palegreen3")) + theme_classic()
rm(trend_cat)
```
The case type categories almost form bands, which meets our anecdotal expectations, with judges treating economic and labor matters much more liberally than criminal justice issues.\

### Limitations  

One final point before we move on. Legal cases involve one or more plaintiffs against one or more defendants contesting a specific factual pattern. While there are obviously trends in the data based on factors outside of the case itself, at the end of the day we will not be able to always determine the outcome of a case without knowing the specific facts. This unknown variability needs to be taken into account when fitting our model.\
\
We also mentioned confounding earlier when looking at political parties, but in fact many of the independent variables may be related or may seem to be associated because underlying factors may be the real cause of both the presentation in the dependent and independent variables.\
\
While I intended to use Linear discriminant analysis (LDA) as one of the models for this project, the data does not fit several of the assumptions required, which is worth mentioning here. Firstly, the data's independent variables do not seem to be normally distributed, with much of the data skewed towards one of the classifiers. Additionally, some of the data sets that contain specific classifiers are too small for the model to function, because there are less data points than degrees of freedom in the model.\
\
I had also originally also wanted to use a k-nearest neighbor approach, but the categorical nature of the data and small number of selected factors compared to the amount of data meant that the distance function used to calculate the neighborhood was too saturated and was causing model failures, even with a neighborhood at $k=1$.\ 

## Data Analysis Conslusions

All of the features we examined look promising as predictors so we will select them all except the judge's political party, which we discussed may be too confounded with other predictors. We will use 8 predictors:\

* Appointing President - appres
* President's Political Party - presparty
* State - state
* Case Topic - casetype
* Case Topic Category - category
* Judge's Gender - gender
* Judge's Ethnicity - race
* Number of Years on the Bench - tenure

```{r include=FALSE}
CMtraining_selected <- CMtraining %>% select(appres, presparty, state, category, gender, race, tenure, casetype, libcon)
CMtest_selected <- CMtest %>% select(appres, presparty, state, category, gender, race, tenure, casetype, libcon)

CMtraining_selected <- CMtraining_selected %>% mutate(libcon=as.factor(libcon))
CMtest_selected <- CMtest_selected %>% mutate(libcon=as.factor(libcon))
```

# Supervised Learning Methods  

## Modeling Approach  

We will build several machine learning algorithms and additionally ensemble them into a final predictor. We will include three models, a linear (logistic) model, a Naive Bayes classifier, and a random forest model. After testing, we will select the most accurate result.\

### Cross Validation of Training Data

Using the same ideas we discussed when we set aside a portion of our data to do unbiased testing, we can use a similar approach when working to tune our model parameters. We can randomly select a portion of our training data to train from, say 85%, model it, and then test it against the 15% of data that was left over. We can do this multiple time to build confidence in our parameters, When the first pass is completed, we replace the sample we used and sample again from the entire training set. This will allow us to find parameters that are more stable and robust than just modeling against the entire training set at once. Randomly selecting the sample from the whole population is a method called "bootstrapping." For these models we will bootstrap the samples with replacement 10 times. This control method is defined and then we can use it for tuning in all of our models.\
```{r}
control <- trainControl(method = "boot", number = 10, p = .85)
```

## Models & Algorithms  

### Linear (Logisitic) Model  

Linear models are simple but are widely used and can be effective. These model presume some form of a linear relationship between the factors and make predictions based on that assumption.\
\
Linear models fit the general form of:

$$\hat Y_{n} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{n}x_{n} + \epsilon_{n}$$

Where $\hat Y$ is the predicted tendency, is $\beta_{0}$ is $\mu$ is the true population mean of the tendency, $\beta_{n}$ is the true bias effects of $x_{n}$, the features selected, and $\epsilon_{n}$ is the error distribution.\
\
The individual bias effect(s) $\beta_{n}$ take for the form of:

$$\hat \beta_{n}=\frac{1}{N}\sum_{i=1}^{N}(y_{n}-\hat \mu)$$

where $\hat \beta_{n}$ is the predicted value of $\beta_{n}$, $N$ is the number of samples, $y_{n}$ is the predicted ruling, and $\hat \mu$ is the predicted mean. Since we aim to measure a particular feature, we treat that each item in that category as a small set. We "take away" our population mean and then calculate the mean across the deviation from that prediction. The mean of the deviations is used to correct our prediction for all items in the category.\
\
If we can combine two or more features together to estimate the bias of further terms:

$$\hat \beta_{n}=\frac{1}{N}\sum_{i=1}^{N}(y_{n}-\hat \beta_{n-1}-\cdots-\hat \beta_{1}-\hat \mu)$$

where $\hat \beta_{n}$ is the prediction for the second factor, $N$ is the number of samples, $y_{n}$ is the predicted ruling, $\hat \beta_{n-1}$, $\hat \beta_{1}$, etc are the predicted bias from the other selected features, and $\hat \mu$ is the predicted mean. We do the exact same math here, picking a second category but this time removing the predicted mean and the bias effect of the first characteristic. After those factors are removed, we are again left with a set of deviations based on a feature and then we can assign the mean of those deviations for that item. We can continue to add terms in this way and we should increase our accuracy each time.\
\
One thing to note here, while we can continue to increase our accuracy for the whole set, at some point the features we select will have such small groups that they might contain only 1 member. This could lead to a model that is so precise it will not be able to account for new variations. This is called 'over-fitting.'\
\
Because we have a Bernoulli distribution, we can use a special case of the linear model called logistical Regression. Logistical regression is a variation of the general linear model specifically used to classify binary events, like the outcome of a case being liberal or conservative. as we have here.\
\
The logistical regression model is defined as follows:

$$p \left(x \right)=P[Y=1 \mid X=x]$$
$$1-p \left(x \right)=P[Y=0 \mid X=x]$$

$$ln{\left(p \left(x_{n} \right)  \over 1-p\left(x_{n} \right) \right)} = \beta_{0} + \beta_{1} x_{1} + . . + \beta_{n} x_{n} $$

Where $p\left(x_{n} \right)$ is the probability that the output is 1 given our set of $x$ independent variables and $\beta_{n}$ is still a bias effect for a given characteristic variable $x_{n}$. This will be our first model.\

### Naive Bayes  

The Naive Bayes is a simple probabilistic classifier and is a popular baseline model for classification. Its most important assumption is that all of the features are *equally important* and *independent.* While this rarely happens in real world data, we will use it here because the assumptions seem to hold somewhat based on our visual inspection.\
\
Given a vector of predictor variable values $x_{}$, the algorithm assigns conditional probabilities for each of the response variable's $Y_{i}$ classes:\

$$p(Y_{i} \vert x_{}) \,.$$

Then we can apply Bayes' rule from probability theory:

$$p(Y_{i} \vert \mathbf{x}) = \frac{p(Y_{i})p(\mathbf{x} \vert Y_{i})}{p(\mathbf{x})} \propto p((Y_{i})p(\mathbf{x} \vert Y_{i})$$

In practice we ignore the denominator, which is a constant in any given analysis.\
\
The next step is to expand $p(\mathbf{x} \vert Y_{i})$:\

$$p(\mathbf{x} \vert Y_{i}) = p(x_1,\ldots,x_n \vert Y_{i})$$
$$p(\mathbf{x} \vert Y_{i}) = p(x_1 \vert x_2,\ldots,x_n,Y_{i}) p(x_2 \vert x_3,\ldots,x_n,Y_{i}) \cdots p(x_n \vert Y_{i})$$

Then we can apply our assumption that the factors are independent of each other:

$$p(x_1 \vert x_2,\ldots,x_n,Y_{i}) p(x_2 \vert x_3,\ldots,x_n,Y_{i}) \cdots p(x_n \vert Y_{i}) \rightarrow p(x_1 \vert Y_{i}) \cdots p(x_n \vert Y_{i})$$

and can simplify:

$$
p(Y_{i} \vert \mathbf{x}) \propto p(Y_{i}) \prod_{n=1}^n p(x_n \vert Y_{i})
$$

We will use this as our second model.

### Randon Forest  

Random Forest is a model built from an ensemble of random decisions trees. Decision trees are predictive models which split the data into various subsets based on characteristics and assigning a probable predicted outcome based on which subset the data point falls into; this process is called recursive partitioning. Visually, this can be represented as nodes with "branches" coming off of them. We can view them as follows:\

$$\hat Y_{i} = \begin{cases} x_{i,1} > m \implies \begin{cases} x_{i,2} > n \implies \begin{cases} x_{i,3} > c \implies 1 \\ x_{i,3} < c \implies \begin{cases} x_{i,4} > s \implies 1 \\ x_{i,4} < s \implies 0 \end{cases} \end{cases} \\ x_{i,2} < n \implies \begin{cases} x_{i,4} > s \implies 1 \\ x_{i,4} < s \implies 0 \end{cases} \end{cases} \\ x_{i,1} < m \implies \begin{cases} x_{i,2} > n \implies \begin{cases} x_{i,4} > s \implies 1 \\ x_{i,4} < s \implies 0 \end{cases} \\ x_{i,n} < n \implies \begin{cases} x_{i,3} > c \implies 1 \\ x_{i,3} < c \implies \begin{cases} x_{i,4} > s \implies 1 \\ x_{i,4} < s \implies 0 \end{cases} \end{cases} \end{cases} \end{cases}$$

Where $\hat Y_i$ is the predicted value and $x_{i,1}$, $x_{i,2}$, $x_{i,3}$, and $x_{i,4}$ are characteristics with values of in the range of $m$, $n$, $c$, and $s$ respectively. Decision trees work by following the "tree" down the "branches"; each time you get to a node you examine a (set of) characteristic(s) that causes you to go down the path one way or the other. You continue following your choices until you get to the end of the chart, also called a "leaf". The leaf will then assign a prediction to the data point.\
\
Random forest generate many random decision trees from the data, allowing the model to be more flexible and avoiding the common problem of over-fitting that can happen if just one tree is created for the model.\

### Ensemble  

Finally, an ensemble model uses predictions from other models to make its prediction. The model looks at all the other predictions and then uses a majority voting approach to determine how it should predict the outcome. If a majority of the models predicts the outcome of the case as liberal, the ensemble will classify it that way as well. We are using our three underlying models and so we will always have a majority and do not need to consider a tie breaker mechanism. We can view the model this way:\

$$\hat Y_{i} = \begin{cases} y_{i,1} + y_{i,2} + y_{i,3} > 1 \implies 1 \\ y_{i,1} + y_{i,2} + y_{i,3} < 2 \implies 0 \end{cases}$$
Where $\hat Y_i$ is the prediction of the ensemble and $y_{i,1}$, $y_{i,2}$, and $y_{i,3}$ are the predictions from our previous models.\

# Results  

## Metrics  

Before we can examine how our models performed, we need to define the metric we will use to compare them. A reason that the legal industry has been slow to adopt artificial intelligence and machine learning is that users expect very accurate predictions and the business will not compromise or allow for a general weakness within models.\
\
Because we are predicting a single outcome as either liberal or conservative, there are 4 possible results from our prediction:\

* True positive (TP): We predict the case is ruled as liberal and the case is actually ruled as liberal
* False positive (FP): We predict the case is ruled as liberal and the case is actually ruled as conservative
* False negative (FN): We predict the case is ruled as conservative and the case is actually ruled as liberal
* True negative (TN): We predict the case is rules as conservative and the case is actually rules as conservative

There are many ways to evaluate this set of outcomes to measure success and we would normally need to be overly cautious in selecting a quality measure based on the demands of the industry. Best practice would be to work with industry experts and find a balance between several metrics, but for simplicity's sake, we will focus on measures that can be expressed as a single value and then we will further narrow down the possibilities to three options for our metric, accuracy, $F_{1}$ score, and Matthews correlation coefficient.\

### Accuracy  

Accuracy measures the total of the correct outcomes over the total number of outcomes:

$$ Accuracy = \frac{ \Sigma TP + \Sigma TN}{\Sigma predictions}$$

Accuracy can be a good metric if the data is balanced overall, meaning that possible outcome are approximately equally likely to occur. If one outcome is much more likely, accuracy can be increased by focusing on the result with the highest prevalence at the expense of the other result. Our earlier examination of the Carp-Manning data shows that conservative outcomes are much more likely overall, so this could be a problem if we selected accuracy as our metric. We need to find a metric that balances the approach to can account for a skew in the result distribution.\

### F1 score  

$F_{1}$ measures the harmonic mean between precision and recall and can be a more balanced approach to performance measurement. We can begin by defining the components and then we will define the metric in full:\

$$ recall = \frac{ \Sigma TP}{\Sigma TP + \Sigma FP} $$
$$ precision = \frac{ \Sigma TP}{\Sigma TP + \Sigma FN} $$
$$ F_{1} = 2\left( \frac{ \left(precision \right) \left(recall \right)}{precision + recall} \right)$$

Which can be simplified to:\

$$ F_{1} = \frac{ \Sigma TP}{ \Sigma  TP+ \frac{1}{2} \left(  \Sigma FP +  \Sigma FN \right)}$$

We notice that while $f_{1}$ is more balanced, it does not take the true negative value into consideration, meaning we can still do better in our hunt for a metric that would work for this use case.\

### Matthews correlation coefficient

The Matthews correlation coefficient (MCC) is regarded as one of the most balanced and complete single value metrics available for data considering a binary outcomes. The metric shows the correlation between predicted and observed values:\

$$ MCC = \frac{ \left( \Sigma TP \right) \left( \Sigma TN \right)- \left( \Sigma FP \right) \left( \Sigma FN \right)}{\sqrt{ \left( \Sigma TP+ \Sigma FP \right) \left(\Sigma TP+\Sigma FN \right) \left(\Sigma TN+ \Sigma FP \right) \left( \Sigma TN+\Sigma FN \right)}}$$

This is the most robust option available to us and because of the skew in our sample outcomes, this is the option we will select.\
\
Because MCC is a coefficient, the possible values range from -1 to 1 with 1 being perfect predictions and 0 being a random guess (a negative score would mean that we were predicting the incorrect answer). The closer the MCC is to 1, the better our model is performing.\
\
we can build a function in r to calculate this from the confusion matrix output from the caret library:\
```{r}
MCC <- function (conf_matrix) {
TP <- conf_matrix$table[1,1]
TN <- conf_matrix$table[2,2]
FP <- conf_matrix$table[1,2]
FN <- conf_matrix$table[2,1]

mcc_num <- (TP*TN - FP*FN)
mcc_den <- as.double((TP+FP))*as.double((TP+FN))*as.double((TN+FP))*as.double((TN+FN))

mcc_final <- ifelse(mcc_den == 0, 0, mcc_num/sqrt(mcc_den))
return(mcc_final)
}
```

## Performance  

With our metric selected, we can now evaluate how our models perform.

### Linear (Logisitic)  

There are no parameters that can be adjusted or tuned in this model, so we will simply run it against our validation cluster and check our results:\
```{r echo=FALSE, cache=TRUE}
train_linear <- train(libcon~., data=CMtraining_selected, method="glm", na.action=na.omit, trControl=control)
y_hat_linear <- predict(train_linear, CMtest_selected)
linear_fit <- confusionMatrix(y_hat_linear, CMtest_selected$libcon)
linearMCC <- MCC(linear_fit)
Results <- tribble(
  ~method, ~MCC,
  "Linear (Logistic)",   linearMCC)
train_linear
```

We can use the results outlined above to calculate our final score:\

```{r echo=FALSE}
Results[1,]
```

This result is promising, but not exceptional. It is a good first effort.\

### Naive Bayes  

The Naive Bayes model has three parameters we can tune in the model, "useKernel", "laplace", and "adjust". "useKernel" allows us to use a kernel density estimate for continuous variables versus a guassian density estimate. Because we have a discrete dependent variable, this will probably perform better as false, but we will test both versions. "Laplace" allows us to incorporate a laplace smoother, which will help us deal with the potential of a zero probably when using factored predictors like we have in our data set. And finally, "adjust" allows us to adjust the bandwidth of the kernel density  with larger numbers meaning a more flexible estimate. 

* usekernel = True or False
* laplace = 0, 0.5, or 1 
* adjust = 0.75, 1, 1.25, or 1.5

We can look to see how this model performed:\

```{r echo=FALSE, cache=TRUE}
nb_grid <-   expand.grid(usekernel = c(TRUE, FALSE),
                        laplace = c(0, 0.5, 1), 
                        adjust = c(0.75, 1, 1.25, 1.5))

train_nb <- train(libcon~., data=CMtraining_selected, method="naive_bayes", na.action=na.omit, trControl=control, tuneGrid=nb_grid)
y_hat_nb <- predict(train_nb, CMtest_selected)
nb_fit <- confusionMatrix(y_hat_nb, CMtest_selected$libcon)
nbMCC <- MCC(nb_fit)
Results <- add_row(Results, method = "Naive Bayes", MCC = nbMCC)
train_nb
```

We will again use the results to calculate the final score:\

```{r echo=FALSE}
Results[2,]
```

This model performed less well than the linear model, but it still a decent improvement over just guessing. The tuning parameters were less impactful than expected. Regardless, it should add value to our ensemble but it will not be selected as the best model.\

### Random Forest  

The Random forest has 2 parameters that can be tuned when testing the model, the first is "predFixed", which determines the minimum number of predictors at any giving split in the tree and "minNode", which determine the minimum number of distinct row references required to allow for a node to split. We can will test several values for each:\

* predFixed = 1, 3, 5 or 7
* minNode = 2, 50, 100

We can see the optimal values and model information here:\

```{r echo=FALSE, cache=TRUE}
rf_grid <-   expand.grid(predFixed = c(3,5,7),
                        minNode = c(50,100))

train_rf <- train(libcon~., data=CMtraining_selected, method="Rborist", na.action=na.omit, trControl=control, tuneGrid=rf_grid)
y_hat_rf <- predict(train_rf, CMtest_selected)
rf_fit <- confusionMatrix(y_hat_rf, CMtest_selected$libcon)
rfMCC <- MCC(rf_fit)
Results <- add_row(Results, method = "Random Forest", MCC = rfMCC)
train_rf
```

The MCC associated with these results is:\

```{r echo=FALSE}
Results[3,]
```

Great! We've improved over the linear fit and have a new top candidate for best fit.\

### Ensemble  

```{r echo=FALSE}
ES <- data.frame(y_hat_linear, y_hat_rf, y_hat_nb)
ES <- ES %>% mutate(totals=(as.integer(y_hat_linear)+ as.integer(y_hat_rf)+ as.integer(y_hat_nb)), y_hat_es=as.factor(ifelse(totals > 4, 1, 0)))
ensemble_fit <- confusionMatrix(ES$y_hat_es, CMtest_selected$libcon)
ensembleMCC <- MCC(ensemble_fit)

ensemble_fit
Results <- add_row(Results, method = "Ensemble", MCC = ensembleMCC)
```

Our final MCC score is:

```{r echo=FALSE}
Results[4,]
```

This turns out to be our second best model, coming in slightly better than than the linear model and well above our last place model, naive Bayes.

# Conclusion  

Our goal was to create a classifier that predicted if a case would have liberal or conservative ruling and we were able to build several models that performed consistently better than guessing when utilizing a handful of diverse variables in the Carp-Manning data set, despite the limitations that we outlined above.\

## Selected model

Our best performing model was the random forest. \

## Future Work

We did not consider several of the variables or even explore all available variables during our visualization step. Future work could utilize additional variables to increase the accuracy of the predictions. Additionally, improvements could be made to the case type categorization system to increase the detail and complexity of the classification system which would increase the model accuracy. Another area of improvement would be to increase our computational power and focus on building prediction profiles for individual judges which could be even more useful than the general approach we have taken.\

## Impact

Even though we were not able to create a model with a high degree of accuracy, our lower quality models could still be impactful in the legal space. This information could be leveraged to help law firms build settlement strategies or help determine which cases the firms should focus its resources on. If the model were even more accurate, this could have a larger impact, but the model will not replace the human act of analyzing facts and applying laws to determine how justice should best be served.
